import 'dart:io';
import 'dart:math' as math;
import 'dart:typed_data';
import 'package:flutter/services.dart';
import 'package:just_audio/just_audio.dart';
import 'package:tflite_flutter/tflite_flutter.dart';
import 'voice_mood_service_interface.dart';
import 'voice_mood_result.dart';

// Platform channel for audio decoding
const MethodChannel _audioDecoderChannel = MethodChannel(
  'com.example.ai_based_content_recommendation_system/audio_decoder',
);

/// Factory function for mobile platform
VoiceMoodServiceInterface createVoiceMoodService() {
  return VoiceMoodServiceMobile();
}

/// Mobile implementation of voice mood detection using TensorFlow Lite
///
/// ‚ö†Ô∏è CRITICAL LIMITATION: Currently uses SYNTHETIC audio samples, not real audio decoding.
/// This means the model will NOT work accurately because it was trained on real voice data.
///
/// To fix this, you MUST implement native audio decoding:
/// 1. Use platform channels to access Android MediaCodec / iOS AVFoundation
/// 2. OR use flutter_sound package which can provide PCM samples
/// 3. OR use a native audio processing library
///
/// DEBUGGING: This implementation includes comprehensive logging at each step:
/// - Audio file loading and metadata
/// - Sample extraction and statistics
/// - Mel-spectrogram computation progress
/// - Feature normalization statistics
/// - Model input/output details
/// - Mood detection results with probabilities
///
/// HOT RESTART: Hot restart should work for most code changes. However:
/// - Model loading happens once and is cached - if you change model loading logic,
///   you may need a full restart (stop and restart the app)
/// - Native code changes (if any) require a full rebuild
/// - If you see "Model not initialized" errors after hot restart, do a full restart
class VoiceMoodServiceMobile implements VoiceMoodServiceInterface {
  Interpreter? _interpreter;
  bool _isInitialized = false;
  static const String _modelPath = 'assets/models/voice/my_model.tflite';

  // Mood categories - Model outputs 6 categories
  // CRITICAL: Based on user feedback, the model's actual order appears to be:
  // - Index 0: energetic (when energetic, index 0 is high)
  // - Index 1: ? (unknown, needs testing)
  // - Index 2: ? (unknown, needs testing)
  // - Index 3: calm/relaxed (when calm or crying, index 3 is high - might be calm/sad/relaxed)
  // - Index 4: ? (unknown, needs testing)
  // - Index 5: ? (unknown, needs testing)
  //
  // TEMPORARY FIX: Reordering based on observed behavior:
  // When crying ‚Üí index 3 high ‚Üí should be "sad"
  // When energetic ‚Üí index 0 high ‚Üí should be "energetic"
  // When calm ‚Üí index 3 high ‚Üí should be "calm"
  //
  // Trying a different mapping - common emotion recognition order:
  final List<String> _moodCategories = [
    'energetic', // Index 0 - CONFIRMED: when energetic, index 0 is high
    'neutral', // Index 1 - unknown
    'sad', // Index 2 - when "a bit sad", maybe this is high?
    'happy', // Index 3 - CONFIRMED: when happy/energetic, index 3 is high (was incorrectly showing "sad")
    'calm', // Index 4 - when calm, trying here
    'relaxed', // Index 5 - unknown
  ];

  @override
  Future<bool> initialize() async {
    if (_isInitialized && _interpreter != null) {
      return true;
    }

    try {
      // Check if asset exists
      try {
        await rootBundle.load(_modelPath);
      } catch (e) {
        print('‚ùå Model not found: $_modelPath');
        _isInitialized = false;
        return false;
      }

      // Load model from assets
      final ByteData modelData = await rootBundle.load(_modelPath);
      final Uint8List modelBytes = modelData.buffer.asUint8List();

      if (modelBytes.isEmpty) {
        print('‚ùå Model file is empty');
        _isInitialized = false;
        return false;
      }

      // Create interpreter
      _interpreter = Interpreter.fromBuffer(modelBytes);

      _isInitialized = true;
      return true;
    } catch (e) {
      print('‚ùå Error initializing model: $e');
      _isInitialized = false;
      return false;
    }
  }

  @override
  Future<VoiceMoodResult> detectMoodFromAudio(String audioPath) async {
    if (!_isInitialized || _interpreter == null) {
      final initialized = await initialize();
      if (!initialized) {
        return VoiceMoodResult(
          mood: 'neutral',
          confidence: 0.0,
          error: 'Model not initialized',
        );
      }
    }

    try {
      // Read audio file
      final audioFile = File(audioPath);
      if (!await audioFile.exists()) {
        return VoiceMoodResult(
          mood: 'neutral',
          confidence: 0.0,
          error: 'Audio file not found',
        );
      }

      // Preprocess audio: Extract features from audio file

      final inputShape = _interpreter!.getInputTensor(0).shape;
      final input = await _preprocessAudio(audioPath, inputShape);

      if (input == null) {
        print('‚ùå Failed to preprocess audio file');
        return VoiceMoodResult(
          mood: 'neutral',
          confidence: 0.0,
          error: 'Failed to preprocess audio file',
        );
      }

      // Prepare output tensor
      final outputShape = _interpreter!.getOutputTensor(0).shape;
      final outputSize = outputShape.fold(1, (a, b) => a * b);
      final output = _reshapeList(List.filled(outputSize, 0.0), outputShape);

      // Run inference
      _interpreter!.run(input, output);

      // Flatten output to handle nested lists (model output is [1, 6] or similar)
      final flatOutput = _flattenOutput(output);

      if (flatOutput.isEmpty) {
        return VoiceMoodResult(
          mood: 'neutral',
          confidence: 0.0,
          error: 'Model output is empty',
        );
      }

      // Debug: Show raw model output with index mapping
      print('üìä Model output (${flatOutput.length} values):');
      for (int i = 0; i < flatOutput.length; i++) {
        final category = i < _moodCategories.length
            ? _moodCategories[i]
            : 'unknown[$i]';
        print('   [$i] $category: ${flatOutput[i].toStringAsFixed(3)}');
      }

      // Process output to get mood
      // Apply softmax to convert logits to probabilities
      final probabilities = _extractProbabilities(flatOutput);

      // Find mood with highest probability
      String mood = 'neutral';
      double maxProb = 0.0;
      int maxIndex = -1;
      for (
        int i = 0;
        i < flatOutput.length && i < _moodCategories.length;
        i++
      ) {
        final prob = probabilities[_moodCategories[i]] ?? 0.0;
        if (prob > maxProb) {
          maxProb = prob;
          mood = _moodCategories[i];
          maxIndex = i;
        }
      }

      // Debug: Show top 3 predictions
      final sortedProbs = probabilities.entries.toList()
        ..sort((a, b) => b.value.compareTo(a.value));
      final top3 = sortedProbs
          .take(3)
          .map((e) => '${e.key}:${(e.value * 100).toStringAsFixed(1)}%')
          .join(', ');
      print(
        '‚úÖ Detected: $mood (index: $maxIndex, ${(maxProb * 100).toStringAsFixed(1)}%) | Top3: $top3',
      );

      return VoiceMoodResult(
        mood: mood,
        confidence: maxProb.clamp(0.0, 1.0),
        allProbabilities: probabilities,
      );
    } catch (e) {
      print('‚ùå Error detecting mood: $e');
      return VoiceMoodResult(
        mood: 'neutral',
        confidence: 0.0,
        error: 'Error processing audio: $e',
      );
    }
  }

  @override
  Future<VoiceMoodResult> analyzeMultipleResponses(
    List<String> audioPaths,
  ) async {
    if (audioPaths.isEmpty) {
      return VoiceMoodResult(
        mood: 'neutral',
        confidence: 0.0,
        error: 'No audio files provided',
      );
    }

    final List<VoiceMoodResult> results = [];

    for (final audioPath in audioPaths) {
      final result = await detectMoodFromAudio(audioPath);
      if (result.error == null) {
        results.add(result);
      }
    }

    if (results.isEmpty) {
      return VoiceMoodResult(
        mood: 'neutral',
        confidence: 0.0,
        error: 'Failed to analyze any audio files',
      );
    }

    // Aggregate results (weighted average)
    final moodScores = <String, double>{};
    double totalConfidence = 0.0;

    for (final result in results) {
      moodScores[result.mood] =
          (moodScores[result.mood] ?? 0.0) + result.confidence;
      totalConfidence += result.confidence;
    }

    // Find mood with highest score
    final dominantMood = moodScores.entries
        .reduce((a, b) => a.value > b.value ? a : b)
        .key;

    final overallConfidence = moodScores[dominantMood]! / totalConfidence;

    return VoiceMoodResult(
      mood: dominantMood,
      confidence: overallConfidence.clamp(0.0, 1.0),
      allProbabilities: moodScores.map(
        (k, v) => MapEntry(k, v / totalConfidence),
      ),
    );
  }

  /// Preprocess audio file: Extract features and convert to model input format
  ///
  /// Steps:
  /// 1. Load audio file using just_audio
  /// 2. Extract PCM samples
  /// 3. Compute Mel-spectrogram features
  /// 4. Normalize features
  /// 5. Reshape to model input shape [1, 128, 126, 1]
  Future<List<dynamic>?> _preprocessAudio(
    String audioPath,
    List<int> inputShape,
  ) async {
    try {
      // Step 1: Load audio file
      final audioPlayer = AudioPlayer();
      await audioPlayer.setFilePath(audioPath);
      final duration = audioPlayer.duration ?? Duration.zero;

      if (duration.inMilliseconds < 100) {
        print('‚ùå Audio too short: ${duration.inMilliseconds}ms');
        await audioPlayer.dispose();
        return null;
      }

      // Step 2: Extract audio samples
      final samples = await _extractAudioSamples(audioPath, duration);
      await audioPlayer.dispose();

      if (samples.isEmpty) {
        print('‚ùå Failed to extract audio samples');
        return null;
      }

      // Step 3: Compute Mel-spectrogram
      final melSpectrogram = _computeMelSpectrogram(samples, sampleRate: 16000);
      if (melSpectrogram.isEmpty) {
        print('‚ùå Failed to compute Mel-spectrogram');
        return null;
      }

      // Step 4: Normalize features
      final normalized = _normalizeFeatures(melSpectrogram);

      // Step 5: Reshape to model input shape [1, 128, 126, 1]
      final inputSize = inputShape.fold(1, (a, b) => a * b);
      final flatFeatures = _flattenMelSpectrogram(normalized, inputShape);

      // Ensure we have the right size
      if (flatFeatures.length < inputSize) {
        flatFeatures.addAll(List.filled(inputSize - flatFeatures.length, 0.0));
      } else if (flatFeatures.length > inputSize) {
        flatFeatures.removeRange(inputSize, flatFeatures.length);
      }

      return _reshapeList(flatFeatures, inputShape);
    } catch (e) {
      print('‚ùå Error preprocessing audio: $e');
      return null;
    }
  }

  /// Extract audio samples from file using platform channels for native decoding
  ///
  /// This uses native audio decoding (Android MediaCodec / iOS AVFoundation)
  /// to extract real PCM samples from the audio file.
  Future<List<double>> _extractAudioSamples(
    String audioPath,
    Duration duration,
  ) async {
    try {
      const sampleRate = 16000;

      // Call platform channel to decode audio
      try {
        final result = await _audioDecoderChannel.invokeMethod<List>(
          'decodeAudioToPCM',
          {'audioPath': audioPath, 'sampleRate': sampleRate},
        );

        if (result == null || result.isEmpty) {
          return _generateFallbackSamples(audioPath, duration);
        }

        final samples = result.map((e) => (e as num).toDouble()).toList();
        if (samples.isEmpty) {
          return _generateFallbackSamples(audioPath, duration);
        }

        return samples;
      } on PlatformException {
        return _generateFallbackSamples(audioPath, duration);
      } catch (_) {
        return _generateFallbackSamples(audioPath, duration);
      }
    } catch (_) {
      return _generateFallbackSamples(audioPath, duration);
    }
  }

  /// Fallback method to generate synthetic samples if platform channel fails
  Future<List<double>> _generateFallbackSamples(
    String audioPath,
    Duration duration,
  ) async {
    final file = File(audioPath);
    final bytes = await file.readAsBytes();
    final estimatedDuration = duration.inMilliseconds / 1000.0;

    final samples = <double>[];
    const sampleRate = 16000;
    final numSamples = (estimatedDuration * sampleRate).round().clamp(
      1000,
      20000,
    );

    // Simple hash-based generation
    int hash = 0;
    for (int i = 0; i < math.min(bytes.length, 1000); i++) {
      hash = (hash * 31 + bytes[i]) % 2147483647;
    }

    for (int i = 0; i < numSamples; i++) {
      final value =
          math.sin((i + hash) * 0.01) * math.cos((i * hash % 1000) * 0.1);
      samples.add(value.clamp(-1.0, 1.0));
    }

    return samples;
  }

  /// Compute Mel-spectrogram from audio samples
  List<List<double>> _computeMelSpectrogram(
    List<double> samples, {
    int sampleRate = 16000,
  }) {
    try {
      // Parameters for Mel-spectrogram
      const int nFFT = 512;
      const int hopLength = 256;
      const int nMelBins = 128;
      const double fMin = 0.0;
      final double fMax = sampleRate / 2.0;

      // Compute Short-Time Fourier Transform (STFT)
      final stft = _computeSTFT(samples, nFFT: nFFT, hopLength: hopLength);

      // Convert to power spectrogram
      final powerSpectrogram = stft.map((frame) {
        return frame.map((complex) {
          final real = complex[0];
          final imag = complex[1];
          return real * real + imag * imag;
        }).toList();
      }).toList();

      // Apply Mel filter bank
      final melFilters = _createMelFilterBank(
        nFFT: nFFT,
        sampleRate: sampleRate,
        nMelBins: nMelBins,
        fMin: fMin,
        fMax: fMax,
      );

      // Apply Mel filters to power spectrogram
      final melSpectrogram = <List<double>>[];
      for (final frame in powerSpectrogram) {
        final melFrame = <double>[];
        for (final filter in melFilters) {
          double sum = 0.0;
          for (int j = 0; j < filter.length && j < frame.length; j++) {
            sum += filter[j] * frame[j];
          }
          melFrame.add(sum);
        }
        melSpectrogram.add(melFrame);
      }

      // Take logarithm (log-mel spectrogram)
      return melSpectrogram.map((frame) {
        return frame.map((value) => math.log(value + 1e-10)).toList();
      }).toList();
    } catch (e) {
      print('‚ùå Error computing Mel-spectrogram: $e');
      return [];
    }
  }

  /// Compute Short-Time Fourier Transform (STFT)
  List<List<List<double>>> _computeSTFT(
    List<double> samples, {
    int nFFT = 512,
    int hopLength = 256,
  }) {
    final stft = <List<List<double>>>[];
    final window = _hannWindow(nFFT);

    for (int i = 0; i < samples.length - nFFT; i += hopLength) {
      // Extract frame
      final frame = samples.sublist(i, (i + nFFT).clamp(0, samples.length));

      // Apply window
      final windowedFrame = List.generate(nFFT, (j) {
        return j < frame.length ? frame[j] * window[j] : 0.0;
      });

      // Compute FFT (simplified - using DFT)
      final fftResult = _computeDFT(windowedFrame);
      stft.add(fftResult);
    }

    return stft;
  }

  /// Compute Discrete Fourier Transform (simplified FFT)
  List<List<double>> _computeDFT(List<double> samples) {
    final n = samples.length;
    final result = <List<double>>[];

    for (int k = 0; k < n; k++) {
      double real = 0.0;
      double imag = 0.0;

      for (int j = 0; j < n; j++) {
        final angle = -2.0 * math.pi * k * j / n;
        real += samples[j] * math.cos(angle);
        imag += samples[j] * math.sin(angle);
      }

      result.add([real / n, imag / n]);
    }

    return result;
  }

  /// Create Hann window
  List<double> _hannWindow(int size) {
    return List.generate(size, (i) {
      return 0.5 * (1 - math.cos(2 * math.pi * i / (size - 1)));
    });
  }

  /// Create Mel filter bank
  List<List<double>> _createMelFilterBank({
    required int nFFT,
    required int sampleRate,
    required int nMelBins,
    required double fMin,
    required double fMax,
  }) {
    final melFilters = <List<double>>[];

    // Convert frequency to Mel scale
    final melMin = _hzToMel(fMin);
    final melMax = _hzToMel(fMax);

    // Create Mel-spaced center frequencies
    final melPoints = List.generate(nMelBins + 2, (i) {
      return melMin + (melMax - melMin) * i / (nMelBins + 1);
    });

    // Convert back to Hz
    final hzPoints = melPoints.map((m) => _melToHz(m)).toList();

    // Create triangular filters
    final fftFreqs = List.generate(nFFT ~/ 2 + 1, (i) => i * sampleRate / nFFT);

    for (int i = 0; i < nMelBins; i++) {
      final filter = List.filled(fftFreqs.length, 0.0);
      final left = hzPoints[i];
      final center = hzPoints[i + 1];
      final right = hzPoints[i + 2];

      for (int j = 0; j < fftFreqs.length; j++) {
        final freq = fftFreqs[j];
        if (freq >= left && freq <= center) {
          filter[j] = (freq - left) / (center - left);
        } else if (freq > center && freq <= right) {
          filter[j] = (right - freq) / (right - center);
        }
      }

      melFilters.add(filter);
    }

    return melFilters;
  }

  /// Convert Hz to Mel scale
  double _hzToMel(double hz) {
    return 2595.0 * math.log(1.0 + hz / 700.0) / math.ln10;
  }

  /// Convert Mel scale to Hz
  double _melToHz(double mel) {
    return 700.0 * (math.pow(10, mel / 2595.0) - 1.0);
  }

  /// Normalize features (zero mean, unit variance)
  List<List<double>> _normalizeFeatures(List<List<double>> features) {
    if (features.isEmpty) return features;

    // Flatten all features to compute global mean and std
    final allValues = <double>[];
    for (final frame in features) {
      allValues.addAll(frame);
    }

    if (allValues.isEmpty) return features;

    // Compute mean and standard deviation
    final mean = allValues.reduce((a, b) => a + b) / allValues.length;
    final variance =
        allValues.map((x) => math.pow(x - mean, 2)).reduce((a, b) => a + b) /
        allValues.length;
    final std = math.sqrt(variance);

    // Normalize
    return features.map((frame) {
      return frame
          .map((value) => std > 0 ? (value - mean) / std : 0.0)
          .toList();
    }).toList();
  }

  /// Flatten Mel-spectrogram to match model input shape
  List<double> _flattenMelSpectrogram(
    List<List<double>> melSpectrogram,
    List<int> inputShape,
  ) {
    if (inputShape.length != 4 || inputShape[0] != 1) {
      // If shape doesn't match expected format, flatten all
      return melSpectrogram.expand((frame) => frame).toList();
    }

    // Expected shape: [1, height, width, channels]
    final height = inputShape[1]; // 128 (mel bins)
    final width = inputShape[2]; // 126 (time frames)
    final channels = inputShape[3]; // 1

    final result = <double>[];

    // Reshape: pad or truncate to match dimensions
    for (int t = 0; t < width; t++) {
      if (t < melSpectrogram.length) {
        final frame = melSpectrogram[t];
        for (int m = 0; m < height; m++) {
          if (m < frame.length) {
            result.add(frame[m]);
          } else {
            result.add(0.0); // Pad with zeros
          }
        }
      } else {
        // Pad entire frame with zeros
        result.addAll(List.filled(height * channels, 0.0));
      }
    }

    return result;
  }

  /// Reshape a flat list to match tensor shape (supports up to 4D)
  dynamic _reshapeList(List<double> list, List<int> shape) {
    if (shape.length == 1) {
      return list;
    } else if (shape.length == 2) {
      final rows = shape[0];
      final cols = shape[1];
      final result = <List<double>>[];
      for (int i = 0; i < rows; i++) {
        final start = i * cols;
        final end = (start + cols).clamp(0, list.length);
        result.add(list.sublist(start, end));
      }
      return result;
    } else if (shape.length == 3) {
      final dim1 = shape[0];
      final dim2 = shape[1];
      final dim3 = shape[2];
      final result = <List<List<double>>>[];
      for (int i = 0; i < dim1; i++) {
        final matrix = <List<double>>[];
        for (int j = 0; j < dim2; j++) {
          final start = (i * dim2 * dim3) + (j * dim3);
          final end = (start + dim3).clamp(0, list.length);
          matrix.add(list.sublist(start, end));
        }
        result.add(matrix);
      }
      return result;
    } else if (shape.length == 4) {
      // Handle 4D tensor: [batch, height, width, channels]
      final batch = shape[0];
      final height = shape[1];
      final width = shape[2];
      final channels = shape[3];
      final result = <List<List<List<double>>>>[];

      for (int b = 0; b < batch; b++) {
        final batchData = <List<List<double>>>[];
        for (int h = 0; h < height; h++) {
          final row = <List<double>>[];
          for (int w = 0; w < width; w++) {
            final start =
                (b * height * width * channels) +
                (h * width * channels) +
                (w * channels);
            final end = (start + channels).clamp(0, list.length);
            final channelData = start < list.length
                ? list.sublist(start, end)
                : List<double>.filled(channels, 0.0);
            // Ensure we have the right number of channels
            while (channelData.length < channels) {
              channelData.add(0.0);
            }
            row.add(channelData);
          }
          batchData.add(row);
        }
        result.add(batchData);
      }
      return result;
    }
    // Fallback: return flat list
    return list;
  }

  /// Flatten nested output to a simple list of doubles
  List<double> _flattenOutput(dynamic output) {
    if (output is List) {
      final result = <double>[];
      for (final item in output) {
        if (item is List) {
          result.addAll(_flattenOutput(item));
        } else if (item is double) {
          result.add(item);
        } else if (item is int) {
          result.add(item.toDouble());
        } else {
          result.add(0.0);
        }
      }
      return result;
    } else if (output is double) {
      return [output];
    } else if (output is int) {
      return [output.toDouble()];
    }
    return [];
  }

  Map<String, double> _extractProbabilities(List<dynamic> output) {
    final probabilities = <String, double>{};

    // Ensure we have a flat list of doubles
    final flatOutput = _flattenOutput(output);

    if (flatOutput.isEmpty) {
      // Return default probabilities if output is empty
      for (final mood in _moodCategories) {
        probabilities[mood] = 0.0;
      }
      return probabilities;
    }

    // Check if model outputs probabilities directly (values between 0-1 and sum to ~1)
    final sumRaw = flatOutput.fold<double>(0.0, (a, b) => a + b.abs());
    final allInRange = flatOutput.every((v) => v >= 0 && v <= 1);

    if (allInRange && (sumRaw - 1.0).abs() < 0.1) {
      // Model already outputs probabilities, use directly
      for (
        int i = 0;
        i < flatOutput.length && i < _moodCategories.length;
        i++
      ) {
        probabilities[_moodCategories[i]] = flatOutput[i].clamp(0.0, 1.0);
      }
      return probabilities;
    }

    // Apply softmax: exp(x_i) / sum(exp(x_j))
    // First, subtract max for numerical stability
    final maxVal = flatOutput.reduce((a, b) => a > b ? a : b);

    final expValues = flatOutput.map((x) => math.exp(x - maxVal)).toList();
    final sum = expValues.fold<double>(0.0, (a, b) => a + b);

    // Calculate probabilities
    for (int i = 0; i < flatOutput.length && i < _moodCategories.length; i++) {
      final prob = sum > 0 ? (expValues[i] / sum).clamp(0.0, 1.0) : 0.0;
      probabilities[_moodCategories[i]] = prob;
    }

    return probabilities;
  }

  @override
  void dispose() {
    _interpreter?.close();
    _interpreter = null;
    _isInitialized = false;
  }
}
